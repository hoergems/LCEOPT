# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =
saveParticles = false

[plugins]
heuristicPlugin = libparkingHeuristicPlugin.so

planningRewardPlugin = libparkingRewardPlugin.so
executionRewardPlugin = libparkingRewardPlugin.so

planningTerminalPlugin = libparkingTerminalPlugin.so
executionTerminalPlugin = libparkingTerminalPlugin.so

planningTransitionPlugin = libparkingTransitionPlugin.so
executionTransitionPlugin = libparkingTransitionPlugin.so

planningObservationPlugin = libparkingObservationPlugin.so
executionObservationPlugin = libparkingObservationPlugin.so

executionInitialBeliefPlugin = libparkingInitialBeliefPlugin.so
planningInitialBeliefPlugin = libparkingInitialBeliefPlugin.so

[initialBeliefOptions]
positionError = 0.175

[transitionPluginOptions]
controlDuration = 1.0
controlErrorElevation = 0.0
controlErrorVelocity = 0.0
controlErrorYaw = 0.0
numIntegrationSteps = 3

[observationPluginOptions]
correctObservationProbability = 0.7

[rewardPluginOptions]
stepPenalty = 1.0
exitReward = 100.0
collisionPenalty = 100.0

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The planning environment SDF
planningEnvironmentPath = Parking3DEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = Parking3DEnvironment.sdf

robotName = Vehicle

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
additionalDimentions = 5
additionalDimensionLimits = [[-100.0, 100.0], [-100.0, 100.0], [-100.0, 100.0], [-100.0, 100.0], [-100.0, 100.0]]

[action]
additionalDimensions = 3
additionalDimensionLimits = [[-8.0, 8.0], [-1.57, 1.57], [-3.5, 3.5]]

[observation]
additionalDimensions = 1
additionalDimensionLimits = [[0, 3]]

[LCEOPT]
numParameterVectors = 350
numTrajectoriesPerParameterVector = 10
numEliteSamplesFactor = 0.1
learningRate = 1.0
initialStdDev = 0.8
policyTreeDepth = 1

maxNumIterations = 0
maxCovarianceSum = 0

particleFilter = bootstrap
minParticleCount = 30000
nEffectiveParticles = 30000

actionType = CONTINUOUS
numObservationEdges = 4

[simulation]
interactive = false
particlePlotLimit = 0
