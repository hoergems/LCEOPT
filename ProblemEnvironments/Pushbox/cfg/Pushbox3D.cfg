# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libpushboxHeuristicPlugin.so

planningRewardPlugin = libpushboxRewardPlugin.so
executionRewardPlugin = libpushboxRewardPlugin.so

planningTerminalPlugin = libpushboxTerminalPlugin.so
executionTerminalPlugin = libpushboxTerminalPlugin.so

planningTransitionPlugin = libpushbox3DTransitionPlugin.so
executionTransitionPlugin = libpushbox3DTransitionPlugin.so

planningObservationPlugin = libpushbox3DObservationPlugin.so
executionObservationPlugin = libpushbox3DObservationPlugin.so

executionInitialBeliefPlugin = libpushbox3DInitialBeliefPlugin.so
planningInitialBeliefPlugin = libpushbox3DInitialBeliefPlugin.so

[initialBeliefOptions]
# The initial XY-coordinates robot and the box
initialStateVec = [5.5, 9.5, 0.0, 5.5, 5.5, 0.0]
initialBoxPositionUncertainty = 2.0

[transitionPluginOptions]
actionUncertainty = 0.0
moveUncertainty = 0.0
#boxSpeedUncertainty = 0.1
boxSpeedUncertainty = 0.05
#boxPositionMoveUncertainty = 0.1
boxPositionMoveUncertainty = 0.05

[observationPluginOptions]
observationUncertainty = 10
#observationUncertainty = 5
observationBuckets = 12

usePositionObservation = false
positionObservationUncertainty = 0.2

[rewardPluginOptions]
moveCost = 10.0
goalReward = 1000.0
collisionPenalty = 1000.0

[map]
sizeX = 10
sizeY = 10

goalPosition = [8.5, 9.5, 0.0]
goalRadius = 0.5

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
#planningEnvironmentPath = Pushbox.sdf

# The execution environment SDF
#executionEnvironmentPath = Pushbox.sdf

#robotName = PushboxRobot
robotName = Default

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
additionalDimensions = 6
additionalDimensionLimits = [[-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0]]

[action]
# Here we construct a 1-dimensional action space with range 1-13. ABT will discretize the action space into 13 discrete actions
additionalDimensions = 3
additionalDimensionLimits = [[-1.0, 1.0], [-1.0, 1.0], [-1.0, 1.0]]

[observation]
additionalDimensions = 3
additionalDimensionLimits = [[0, 200], [0, 200], [0, 200]]
#additionalDimensions = 6
#additionalDimensionLimits = [[-100, 100], [-100, 100], [-100, 100], [-100, 100], [-100, 100], [-100, 100]]

[LCEOPT]
numParameterVectors = 612
numTrajectoriesPerParameterVector = 31
numEliteSamplesFactor = 0.17
policyTreeDepth = 1.4
learningRate = 0.76
initialStdDev = 0.8

particleFilter = bootstrap
minParticleCount = 100000
nEffectiveParticles = 20000

maxNumIterations = 0
maxCovarianceSum = 0

# For action sequence
parameterVectorLength = 3

# For policy tree
numObservationEdges = 288
#numObservationEdges = 72

actionType = CONTINUOUS

[simulation]
interactive = false
particlePlotLimit = 50
