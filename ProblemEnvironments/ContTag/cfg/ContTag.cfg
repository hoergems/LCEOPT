# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix = 30_25
saveParticles = false

[plugins]
heuristicPlugin = libcontTagHeuristicPlugin.so

planningRewardPlugin = libcontTagRewardPlugin.so
executionRewardPlugin = libcontTagRewardPlugin.so

planningTerminalPlugin = libcontTagTerminalPlugin.so
executionTerminalPlugin = libcontTagTerminalPlugin.so

planningTransitionPlugin = libcontTagTransitionPlugin.so
executionTransitionPlugin = libcontTagTransitionPlugin.so

planningObservationPlugin = libcontTagObservationPlugin.so
executionObservationPlugin = libcontTagObservationPlugin.so

executionInitialBeliefPlugin = libcontTagInitialBeliefPlugin.so
planningInitialBeliefPlugin = libcontTagInitialBeliefPlugin.so

[problem]
robotName = Agent

# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 90

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = ContTag.sdf

# The execution environment SDF
executionEnvironmentPath = ContTag.sdf

[map]
sizeX = 10
sizeY = 5
mapPath = /home/marcus/PhD/scripts/POMDPProblems/ContTag/cfg/map_tag.txt

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
additionalDimensions = 6
additionalDimensionLimits = [[-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10]]

[action]
additionalDimensions = 2

# Encodes 13 discrete actions
additionalDimensionLimits = [[0, 1], [0, 1]]

[observation]
additionalDimensions = 1
additionalDimensionLimits = [[0, 1]]

[LCEOPT]
numParameterVectors = 493.2569
numTrajectoriesPerParameterVector = 103.2346
numEliteSamplesFactor = 0.0453
policyTreeDepth = 2.2954
learningRate = 0.7034

particleFilter = bootstrap
minParticleCount = 30000
nEffectiveParticles = 30000

maxNumIterations = 0
maxCovarianceSum = 0

# For action sequence
parameterVectorLength = 3

# For policy tree
numObservationEdges = 2

initialStdDev = 1.5559

actionType = CONTINUOUS

[simulation]
interactive = false
particlePlotLimit = 200
