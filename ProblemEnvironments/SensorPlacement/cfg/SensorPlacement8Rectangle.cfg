# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libsensorPlacementHeuristicPlugin.so

planningRewardPlugin = libsensorPlacementRewardPlugin.so
executionRewardPlugin = libsensorPlacementRewardPlugin.so

planningTerminalPlugin = libsensorPlacementTerminalPlugin.so
executionTerminalPlugin = libsensorPlacementTerminalPlugin.so

planningTransitionPlugin = libsensorPlacementTransitionPlugin.so
executionTransitionPlugin = libsensorPlacementTransitionPlugin.so

planningObservationPlugin = libsensorPlacementObservationPlugin.so
executionObservationPlugin = libsensorPlacementObservationPlugin.so

executionInitialBeliefPlugin = libsensorPlacementInitialBeliefPlugin.so
planningInitialBeliefPlugin = libsensorPlacementInitialBeliefPlugin.so

[initialBeliefOptions]
# The initial joint angles
initialStateVec = [0.0, -1.57, 1.57, 0.0, 0.0, 0.0, 0.0, 0.0]
lowerUpperBound = [0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075]


[transitionPluginOptions]
endEffectorLink = 8DOFManipulator::endEffector
#upperTransitionErrorBound = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]

lowerTransitionErrorBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
upperTransitionErrorBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

[observationPluginOptions]
correctObservationProbability = 1.0

[rewardPluginOptions]
stepPenalty = 1.0
illegalMovePenalty = 500.0
exitReward = 1000

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = 8DOFSensorPlacementEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = 8DOFSensorPlacementEnvironment.sdf

robotName = 8DOFManipulator

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
jointPositions = [8DOFManipulator::joint1, 8DOFManipulator::joint2, 8DOFManipulator::joint3, 8DOFManipulator::joint4, 8DOFManipulator::joint5, 8DOFManipulator::joint6, 8DOFManipulator::joint7, 8DOFManipulator::joint8]

[action]
# Here we construct a 1-dimensional action space with range 1-13. ABT will discretize the action space into 13 discrete actions
additionalDimensions = 8
additionalDimensionLimits = [[-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5]]


[observation]
additionalDimensions = 1
additionalDimensionLimits = [[0.0, 1.0]]

[ADVT]
explorationFactor = 10.0
explorationFactorDiameter = 10.0
globalPartitionTree = false
numDiameterSamples = 6

# FOR RECTANGLE
splitExplorationFactor = 0.5
minimumSplittingDiam = 1e-5

partitioningMode = RECTANGLE

bellmanBackup = true
resetTree = false
maxObservationDistance = 0.1
minParticleCount = 30000
numEpisodes = 0
maximumDepth = 3
rejectionSampling = false

[POMCPOW]
minParticleCount = 30000
ucbExplorationFactor = 80.0
kA = 30.0
alphaA = 0.4
kO = 25.0
alphaO = 0.4
maximumDepth = 3
rejectionSampling = false
resetTree = false
useVOO = true
vooExplorationFactor = 0.8
observationType = discrete
maxObservationDistance = 0.1
numEpisodes = 0

[ABTPW]
minParticleCount = 10000
rejectionSampling = false
kA = 20.0
alphaA = 0.18
ucbExplorationFactor = 200.0
maxObservationDistance = 0.1
numEpisodes = 0
maximumDepth = 2
actionSampler = voo
vooExplorationFactor = 0.7

[simulation]
interactive = true
particlePlotLimit = 0
