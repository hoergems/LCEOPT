# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libsensorPlacementHeuristicPlugin.so

planningRewardPlugin = libsensorPlacementRewardPlugin.so
executionRewardPlugin = libsensorPlacementRewardPlugin.so

planningTerminalPlugin = libsensorPlacementTerminalPlugin.so
executionTerminalPlugin = libsensorPlacementTerminalPlugin.so

planningTransitionPlugin = libsensorPlacementTransitionPluginKin.so
executionTransitionPlugin = libsensorPlacementTransitionPluginKin.so

planningObservationPlugin = libsensorPlacementObservationPlugin.so
executionObservationPlugin = libsensorPlacementObservationPlugin.so

executionInitialBeliefPlugin = libsensorPlacementInitialBeliefPlugin.so
planningInitialBeliefPlugin = libsensorPlacementInitialBeliefPlugin.so

[initialBeliefOptions]
initialStateVec = [0.0, -1.57, 1.57, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
lowerUpperBound = [0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075]

[transitionPluginOptions]
manipulatorURDF = 10DOFManipulator.urdf
lowerTransitionErrorBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
upperTransitionErrorBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

[observationPluginOptions]
correctObservationProbability = 1.0

[rewardPluginOptions]
stepPenalty = 1.0
illegalMovePenalty = 500.0
exitReward = 1000

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 50

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

# The planning environment SDF
planningEnvironmentPath = 10DOFSensorPlacementEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = 10DOFSensorPlacementEnvironment.sdf

robotName = 10DOFManipulator

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
jointPositions = [10DOFManipulator::joint1, 10DOFManipulator::joint2, 10DOFManipulator::joint3, 10DOFManipulator::joint4, 10DOFManipulator::joint5, 10DOFManipulator::joint6, 10DOFManipulator::joint7, 10DOFManipulator::joint8, 10DOFManipulator::joint9, 10DOFManipulator::joint10]

[action]
additionalDimensions = 10
additionalDimensionLimits = [[-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5]]

[observation]
additionalDimensions = 1
additionalDimensionLimits = [[0.0, 1.0]]

[LCEOPT]
numParameterVectors = 500
numTrajectoriesPerParameterVector = 5
numEliteSamplesFactor = 0.05
learningRate = 1.0
initialStdDev = 0.1

maxNumIterations = 0
maxCovarianceSum = 0

# For action sequence
#parameterVectorLength = 3

# For policy tree
numObservationEdges = 5
policyTreeDepth = 1

actionType = CONTINUOUS

particleFilter = bootstrap
minParticleCount = 30000
nEffectiveParticles = 30000

[simulation]
interactive = false
particlePlotLimit = 0
